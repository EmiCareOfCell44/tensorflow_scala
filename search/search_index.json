{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/architecture.html","text":"","title":"Architecture"},{"location":"/architecture.html#architecture","text":"","title":"Architecture"},{"location":"/architecture.html#build-configuration","text":"SBT commands: - clean / compile / nativeCompile / test: Commands that will compile and run everything for the local system. - cross:* / cross:nativeCrossCompile: Commands that cross-compile native code using Docker container. - publish / publishLocal: Never cross-compile. - publishCrossCompiled / publishLocalCrossCompiled: For that purpose use these instead.","title":"Build Configuration"},{"location":"/guides/graph_collections.html","text":"","title":"Graph Collections"},{"location":"/guides/graph_collections.html#graph-collections","text":"","title":"Graph Collections"},{"location":"/guides/graph_collections.html#creating-a-new-graph-collection-key","text":"Create a key that extends the Graph.Key[T] trait, where T is the type of values the corresponding collection holds.\nNote that, given the complexity that may be involved in serializing certain value types, a few helper sub-traits of Graph.Key[T] are provided, that you can extend and avoid having to write the serialization code. Those are:\nStringCollectionKey extends Key[String] IntCollectionKey extends Key[Int] OpCollectionKey extends Key[Op] OutputCollectionKey extends Key[Output] VariableCollectionKey extends Key[Variable] SaverCollectionKey extends Key[Saver] ResourceCollectionKey extends Key[Resource]\nIf extending one of these traits, your key implementation can look as simple as this:\nscala object GLOBAL_VARIABLES extends VariableCollectionKey { override def name: String = \"variables\" }\nNote that the helper sub-traits are also compatible with loading/saving from/to code that uses the TensorFlow Python API.\nRegister the new key in a static code block that will be called when your library is coded. For example, if you are contributing within the TensorFlow for Scala API package, add a call such as:\nGraph.Keys.register(Graph.Keys.GLOBAL_VARIABLES)\nin the api package object (i.e., org/platanios/tensorflow/api/package.scala).","title":"Creating a new graph collection key"},{"location":"/index.html","text":"","title":"TensorFlow Scala"},{"location":"/index.html#tensorflow-scala","text":"This library is a Scala API for https://www.tensorflow.org. It attempts to provide most of the functionality provided by the official Python API, while at the same type being strongly-typed and adding some new features. It is a work in progress and a project I started working on for my personal research purposes. Much of the API should be relatively stable by now, but things are still likely to change.","title":"TensorFlow Scala"},{"location":"/index.html#main-features","text":"Easy manipulation of tensors and computations involving tensors (similar to NumPy in Python): val t1 = Tensor(1.2, 4.5)\nval t2 = Tensor(-0.2, 1.1)\nt1 + t2 == Tensor(1.0, 5.6) Low-level graph construction API, similar to that of the Python API, but strongly typed wherever possible: val inputs      = tf.placeholder[Float](Shape(-1, 10))\nval outputs     = tf.placeholder[Float](Shape(-1, 10))\nval predictions = tf.nameScope(\"Linear\") {\n  val weights = tf.variable[Float](\"weights\", Shape(10, 1), tf.ZerosInitializer)\n  tf.matmul(inputs, weights)\n}\nval loss        = tf.sum(tf.square(predictions - outputs))\nval optimizer   = tf.train.AdaGrad(1.0f)\nval trainOp     = optimizer.minimize(loss) Numpy-like indexing/slicing for tensors. For example: tensor(2 :: 5, ---, 1) // is equivalent to numpy's 'tensor[2:5, ..., 1]' High-level API for creating, training, and using neural networks. For example, the following code shows how simple it is to train a multi-layer perceptron for MNIST using TensorFlow for Scala. Here we omit a lot of very powerful features such as summary and checkpoint savers, for simplicity, but these are also very simple to use. // Load and batch data using pre-fetching.\nval dataset = MNISTLoader.load(Paths.get(\"/tmp\"))\nval trainImages = tf.data.datasetFromTensorSlices(dataset.trainImages.toFloat)\nval trainLabels = tf.data.datasetFromTensorSlices(dataset.trainLabels.toLong)\nval trainData =\n  trainImages.zip(trainLabels)\n      .repeat()\n      .shuffle(10000)\n      .batch(256)\n      .prefetch(10)\n\n// Create the MLP model.\nval input = Input(FLOAT32, Shape(-1, 28, 28))\nval trainInput = Input(INT64, Shape(-1))\nval layer = Flatten[Float](\"Input/Flatten\") >>\n    Linear[Float](\"Layer_0\", 128) >> ReLU[Float](\"Layer_0/Activation\", 0.1f) >>\n    Linear[Float](\"Layer_1\", 64) >> ReLU[Float](\"Layer_1/Activation\", 0.1f) >>\n    Linear[Float](\"Layer_2\", 32) >> ReLU[Float](\"Layer_2/Activation\", 0.1f) >>\n    Linear[Float](\"OutputLayer\", 10)\nval loss = SparseSoftmaxCrossEntropy[Float, Long, Float](\"Loss\") >>\n    Mean(\"Loss/Mean\")\nval optimizer = tf.train.GradientDescent(1e-6f)\nval model = Model.simpleSupervised(input, trainInput, layer, loss, optimizer)\n\n// Create an estimator and train the model.\nval estimator = InMemoryEstimator(model)\nestimator.train(() => trainData, StopCriteria(maxSteps = Some(1000000))) And by changing a few lines to the following code, you can get checkpoint capability, summaries, and seamless integration with TensorBoard: val loss = SparseSoftmaxCrossEntropy[Float, Long, Float](\"Loss\") >>\n    Mean(\"Loss/Mean\") >>\n    ScalarSummary(name = \"Loss\", tag = \"Loss\")\nval summariesDir = Paths.get(\"/tmp/summaries\")\nval estimator = InMemoryEstimator(\n  modelFunction = model,\n  configurationBase = Configuration(Some(summariesDir)),\n  trainHooks = Set(\n    SummarySaver(summariesDir, StepHookTrigger(100)),\n    CheckpointSaver(summariesDir, StepHookTrigger(1000))),\n  tensorBoardConfig = TensorBoardConfig(summariesDir))\nestimator.train(() => trainData, StopCriteria(maxSteps = Some(100000))) If you now browse to https://127.0.0.1:6006 while training, you can see the training progress: Efficient interaction with the native library that avoids unnecessary copying of data. All tensors are created and managed by the native TensorFlow library. When they are passed to the Scala API (e.g., fetched from a TensorFlow session), we use a combination of weak references and a disposing thread running in the background. Please refer to tensorflow/src/main/scala/org/platanios/tensorflow/api/utilities/Disposer.scala, for the implementation.","title":"Main Features"},{"location":"/index.html#tutorials","text":"Object Detection using Pre-Trained Models","title":"Tutorials"},{"location":"/index.html#funding","text":"Funding for the development of this library has been generously provided by the following sponsors:\nCMU Presidential Fellowship National Science Foundation Air Force Office of Scientific Research awarded to Emmanouil Antonios Platanios Grant #: IIS1250956 Grant #: FA95501710218","title":"Funding"},{"location":"/installation.html","text":"","title":"Installation"},{"location":"/installation.html#installation","text":"TensorFlow for Scala is currently available for Scala 2.11.x and for 2.12.x. The main line of development is version 2.12.7. Binary release artifacts are published to the Sonatype OSS Repository Hosting service and synced to Maven Central. Currently, given the beta status of this project, only snapshot releases are published.","title":"Installation"},{"location":"/installation.html#library-dependencies","text":"To include the Sonatype repositories in your SBT build and use TensorFlow for Scala, you should add the following dependency:\nsbt libraryDependencies += \"org.platanios\" % \"tensorflow\" % \"0.4.0\" Maven <dependency>\n  <groupId>org.platanios</groupId>\n  <artifactId>tensorflow</artifactId>\n  <version>0.4.0</version>\n</dependency> Gradle dependencies {\n  compile group: 'org.platanios', name: 'tensorflow', version: '0.4.0'\n}\nNote This requires that you have installed the TensorFlow dynamic library in your system. If you haven’t, please continue reading into the following section.","title":"Library Dependencies"},{"location":"/installation.html#dependencies","text":"","title":"Dependencies"},{"location":"/installation.html#tensorflow-dynamic-library","text":"TensorFlow for Scala is an API for TensorFlow. In order for it work, it requires that you have the main TensorFlow dynamic library installed. You have two options for dealing with this requirement:","title":"TensorFlow Dynamic Library"},{"location":"/installation.html#using-precompiled-binaries","text":"Add the following dependency, instead of the previous one:\nsbt libraryDependencies += \"org.platanios\" % \"tensorflow\" % \"0.4.0\" classifier \"linux-cpu-x86_64\" Maven <dependency>\n  <groupId>org.platanios</groupId>\n  <artifactId>tensorflow</artifactId>\n  <version>0.4.0</version>\n  <classifier>linux-cpu-x86_64</classifier>\n</dependency> Gradle dependencies {\n  compile group: 'org.platanios', name: 'tensorflow', version: '0.4.0', classifier: 'linux-cpu-x86_64'\n}\nOperating System Make sure to replace linux-cpu-x86_64 with the string that corresponds to your platform.* Currently supported platforms are: linux-cpu-x86_64, linux-gpu-x86_64, and darwin-cpu-x86_64.","title":"Using Precompiled Binaries"},{"location":"/installation.html#compiling-tensorflow-from-scratch","text":"Compile the TensorFlow dynamic library yourself and install it in your system. This is the recommended approach if you care about performance, but it is also significantly more complicated and time consuming.\nFirst, clone the TensorFlow repository:\ngit clone https://github.com/tensorflow/tensorflow.git <repository_directory>\ncd <repository_directory>\ngit checkout r1.12\nThen, compile TensorFlow using the following commands:\n./configure\nbazel build --config=opt --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 //tensorflow:libtensorflow.so\nFor details regarding the configuration options (e.g., GPU support), please refer to the relevant main TensorFlow documentation page.\nFinally, copy the bazel-bin/tensorflow/libtensorflow.so file (possibly having a different extension, depending on the platform you’re using) file in a directory that is in LD_LIBRARY_PATH, or set LD_LIBRARY_PATH appropriately.\nCompiling TensorFlow Scala If you want to compile TensorFlow for Scala yourself and the libtensorflow.so file is not placed in one of the default system libraries directories, then set -Djava.library.path=<directory> (replacing <directory> with the directory containing the libtensorflow.so file) in the .jvmopts file at the root of the TensorFlow for Scala repository.","title":"Compiling TensorFlow from Scratch"},{"location":"/installation.html#protocol-buffers-compiler","text":"TensorFlow for Scala also requires protoc, the Protocol Buffers compiler (at least version 3), to be installed. You also have two options for dealing with this requirement.","title":"Protocol Buffers Compiler"},{"location":"/installation.html#using-precompiled-binaries","text":"Download pre-built binaries from https://github.com/google/protobuf/releases/ (choose the protoc variant appropriate for your platform) and make sure that protoc is in the PATH (either by installing it in a location in the PATH, or by adding its location to the PATH).","title":"Using Precompiled Binaries"},{"location":"/installation.html#installing-using-a-package-manager","text":"Install it using a package manager:\nOn Debian/Ubuntu, you can install it with APT, using the following command: apt-get install protobuf-compiler On Mac, you can install with Homebrew, using the following command: brew install protobuf","title":"Installing Using a Package Manager"},{"location":"/guides.html","text":"","title":"Guides"},{"location":"/guides.html#guides","text":"Similar to the TensorFlow Python API, by Google, TensorFlow for Scala provides multiple APIs. The lowest level API – Core API – provides you with complete programming control. the core API is suitable for machine learning researchers and others who require fine levels of control over their models. The higher level APIs are built on top of the Core API. These higher level APIs are typically easier to learn and use. In addition, the higher level APIs make repetitive tasks easier and more consistent between different users. A high-level API like the Learn API helps you manage datasets, models, (distributed) training, and inference.\nThe main APIs of TensorFlow Scala are the following:\nThe fact that this library is statically-typed is mentioned a couple times in the above paragraph and that’s because it is a very important feature. It means that many problems with the code you write will show themselves at compile time, which means that your chances of running into the experience of waiting for a neural network to train for a week only to find out that your evaluation code crashed and you lost everything, decrease significantly.\nIt is recommended to first go through the Tensors guide, and then go from high-level to low-level concepts as you progress (i.e., read through the High-Level Learning guide first and then through the Graph Construction guide). Concepts such as the TensorFlow graph and sessions only appear in the Graph Construction guide.\nRelationship to the TensorFlow Python API These guides borrow a lot of material from the official Python API documentation of TensorFlow and adapt it for the purposes of TensorFlow Scala. They also introduce a lot of new constructs specific to this library.","title":"Guides"},{"location":"/guides/tensors.html","text":"","title":"Tensors"},{"location":"/guides/tensors.html#tensors","text":"TensorFlow, as the name indicates, is a framework to define and run computations involving tensors. A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of some underlying data type. A Tensor has a DataType (e.g., FLOAT32 which corresponds to 32-bit floating point numbers) and a Shape (that is, the number of dimensions it has and the size of each dimension – e.g., Shape(10, 2) which corresponds to a matrix with 10 rows and 2 columns) associated with it. Each element in the Tensor has the same data type. For example, the following code creates an integer tensor filled with zeros with shape [2, 5] (i.e., a two-dimensional array holding integer values, where the first dimension size is 2 and the second is 5):\nval tensor = Tensor.zeros[Int](Shape(2, 5))\nYou can print the contents of a tensor as follows:\ntensor.summarize(flattened = true)","title":"Tensors"},{"location":"/guides/tensors.html#tensor-creation","text":"Tensors can be created using various constructors defined in the Tensor companion object. For example:\nval a = Tensor(1, 2)      // Creates a Tensor[Int] with shape [2]\nval b = Tensor(1L, 2)     // Creates a Tensor[Long] with shape [2]\nval c = Tensor(3.0f)      // Creates a Tensor[Float] with shape [1]\nval d = Tensor(-4.0)      // Creates a Tensor[Double] with shape [1]\nval e = Tensor.empty[Int] // Creates an empty Tensor[Int] with shape [0]\nval z = Tensor.zeros[Float](Shape(5, 2))   // Creates a zeros Tensor[Float] with shape [5, 2]\nval r = Tensor.randn(Double, Shape(10, 3)) // Creates a Tensor[Double] with shape [10, 3] and\n                                           // elements drawn from the standard Normal distribution.","title":"Tensor Creation"},{"location":"/guides/tensors.html#data-type","text":"As already mentioned, tensors have a data type. Various numeric data types are supported, as well as strings (i.e., tensors containing strings are supported). It is not possible to have a [Tensor][tensor] with more than one data type. It is possible, however, to serialize arbitrary data structures as strings and store those in [Tensor][tensor]s.\nThe list of all supported data types is:\nSTRING: String. BOOLEAN: Boolean. FLOAT16: 16-bit half-precision floating-point. FLOAT32: 32-bit single-precision floating-point. FLOAT64: 64-bit double-precision floating-point. BFLOAT16: 16-bit truncated floating-point. COMPLEX64: 64-bit single-precision complex. COMPLEX128: 128-bit double-precision complex. INT8: 8-bit signed integer. INT16: 16-bit signed integer. INT32: 32-bit signed integer. INT64: 64-bit signed integer. UINT8: 8-bit unsigned integer. UINT16: 16-bit unsigned integer. QINT8: Quantized 8-bit signed integer. QINT16: Quantized 16-bit signed integer. QINT32: Quantized 32-bit signed integer. QUINT8: Quantized 8-bit unsigned integer. QUINT16: Quantized 16-bit unsigned integer. RESOURCE: Handle to a mutable resource. VARIANT: Variant.\nIt is also possible to cast [Tensor][tensor]s from one data type to another using the toXXX operator, or the castTo[XXX] operator:\nval floatTensor = Tensor[Float](1, 2, 3) // Floating point vector containing the elements: 1.0f, 2.0f, and 3.0f.\nfloatTensor.toInt                        // Integer vector containing the elements: 1, 2, and 3.\nfloatTensor.castTo[Int]                  // Integer vector containing the elements: 1, 2, and 3.\nNOTE: In general, all tensor-supported operations can be accessed as direct methods/operators of the [Tensor][tensor] object, or as static methods defined in the tfi package, which stands for TensorFlow Imperative (given the imperative nature of that API).\nA [Tensor][tensor]’s data type can be inspected using:\nfloatTensor.dataType // Returns FLOAT32\nWhen creating a [Tensor][tensor] from a Scala objects you may optionally specify the data type. If you don’t, TensorFlow chooses a data type that can represent your data. It converts Scala integers to INT32 and Scala floating point numbers to either FLOAT32 or FLOAT64 depending on their precision.\nTensor(1, 2, 3)      // INT32 tensor\nTensor(1, 2L, 3)     // INT64 tensor\nTensor(2.4f, -0.1f)  // FLOAT32 tensor\nTensor(0.6f, 1.0)    // FLOAT64 tensor","title":"Data Type"},{"location":"/guides/tensors.html#rank","text":"The rank of a [Tensor][tensor] is its number of dimensions. Synonyms for rank include order or degree or n-dimension. Note that rank in TensorFlow is not the same as matrix rank in mathematics. As the following table shows, each rank in TensorFlow corresponds to a different mathematical entity:\nRank Math Entity 0 Scalar (magnitude only) 1 Vector (magnitude and direction) 2 Matrix (table of numbers) 3 3-Tensor (cube of numbers) n n-Tensor (you get the idea)\nFor example:\nval t0 = Tensor.ones(INT32, Shape())     // Creates a scalar equal to the value 1\nval t1 = Tensor.ones(INT32, Shape(10))   // Creates a vector with 10 elements, all of which are equal to 1\nval t2 = Tensor.ones(INT32, Shape(5, 2)) // Creates a matrix with 5 rows with 2 columns\n\n// You can also create tensors in the following way:\nval t3 = Tensor(2.0, 5.6)                                 // Creates a vector that contains the numbers 2.0 and 5.6\nval t4 = Tensor(Tensor(1.2f, -8.4f), Tensor(-2.3f, 0.4f)) // Creates a matrix with 2 rows and 2 columns\nA rank of a tensor can be obtained in one of two ways:\nt4.rank      // Returns the value 2\ntfi.rank(t4) // Also returns the value 2","title":"Rank"},{"location":"/guides/tensors.html#shape","text":"","title":"Shape"},{"location":"/guides/tensors.html#indexing-slicing","text":"Similar to NumPy, tensors can be indexed/sliced in various ways:\nAn indexer can be one of: - Ellipsis: Corresponds to a full slice over multiple dimensions of a tensor. Ellipses are used to represent zero or more dimensions of a full-dimension indexer sequence. - NewAxis: Corresponds to the addition of a new dimension. - Slice: Corresponds to a slice over a single dimension of a tensor.\nExamples of constructing and using indexers are provided in the Ellipsis and the Slice class documentation. Here we provide examples of indexing over tensors using indexers:\nval t = Tensor.zeros[Float](Shape(4, 2, 3, 8))\nt(::, ::, 1, ::)            // Tensor with shape [4, 2, 1, 8]\nt(1 :: -2, ---, 2)          // Tensor with shape [1, 2, 3, 1]\nt(---)                      // Tensor with shape [4, 2, 3, 8]\nt(1 :: -2, ---, NewAxis, 2) // Tensor with shape [1, 2, 3, 1, 1]\nt(1 ::, ---, NewAxis, 2)    // Tensor with shape [3, 2, 3, 1, 1]\nwhere --- corresponds to an ellipsis.\nNote that each indexing sequence is only allowed to contain at most one Ellipsis. Furthermore, if an ellipsis is not provided, then one is implicitly appended at the end of indexing sequence. For example, foo(2 :: 4) is equivalent to foo(2 :: 4, ---).","title":"Indexing / Slicing"},{"location":"/guides/graph_construction.html","text":"","title":"Graph Construction"},{"location":"/guides/graph_construction.html#graph-construction","text":"The low level API can be used to define computations that will be executed at a later point, and potentially execute them. It can also be used to create custom layers for the Learn API. The main type of object underlying the low level API is the [Output][output], which represents the value of a [Tensor][tensor] that has not yet been computed. Its name comes from the fact that it represents the output of some computation. An [Output][output] object thus represents a partially defined computation that will eventually produce a value. Core TensorFlow programs work by first building a graph of [Output][output] objects, detailing how each output is computed based on the other available outputs, and then by running parts of this graph to achieve the desired results.\nSimilar to a [Tensor][tensor], each element in an [Output][output] has the same data type, and the data type is always known. However, the shape of an [Output][output] might be only partially known. Most operations produce tensors of fully-known shapes if the shapes of their inputs are also fully known, but in some cases it’s only possible to find the shape of a tensor at graph execution time.\nIt is important to understand the main concepts underlying the core API:\nTensor: Output: Sparse Output: Placeholder: Variable: Graph: Session:\nWith the exception of [Variable][variable]s, the value of outputs is immutable, which means that in the context of a single execution, outputs only have a single value. However, evaluating the same output twice can result in different values. For example, that tensor may be the result of reading data from disk, or generating a random number.","title":"Graph Construction"},{"location":"/guides/graph_construction.html#graph","text":"","title":"Graph"},{"location":"/guides/graph_construction.html#working-with-outputs","text":"","title":"Working with Outputs"},{"location":"/guides/graph_construction.html#evaluating-outputs","text":"","title":"Evaluating Outputs"},{"location":"/guides/graph_construction.html#printing-outputs","text":"","title":"Printing Outputs"},{"location":"/guides/graph_construction.html#logging","text":"Logging in the native TensorFlow library can be controlled by setting the TF_CPP_MIN_LOG_LEVEL environment variable:\n0: Debug level (default). 1: Warning level. 2: Error level. 3: Fatal level.","title":"Logging"},{"location":"/guides/learn.html","text":"","title":"High-Level Learning"},{"location":"/guides/learn.html#high-level-learning","text":"","title":"High-Level Learning"},{"location":"/contributing.html","text":"","title":"Contributing"},{"location":"/contributing.html#contributing","text":"It would be awesome if people could contribute to this library. Given its scope and its early state, before I settle on the API for some of the features, I would really appreciate contributions on the following:\nUnit Tests: Currently unit tests are missing for a big part of the library and it would be extremely useful if we had those. Examples: Examples of code using the library would be great and would also make issues come up early so they can be fixed.","title":"Contributing"},{"location":"/release_notes.html","text":"","title":"Release Notes"},{"location":"/release_notes.html#release-notes","text":"Release 0.4.0 Release 0.3.0 Release 0.2.4 Release 0.2.3 Release 0.2.2 Release 0.2.1 Release 0.2.0 Release 0.1.1 Release 0.1.0","title":"Release Notes"},{"location":"/release_notes/0.4.0.html","text":"","title":"Release 0.4.0"},{"location":"/release_notes/0.4.0.html#release-0-4-0","text":"This is a major release with a lot of new features related to static types for tensors and ops. The graph construction API is now statically-typed, thus enabling much better type safety than before.\nTensors and outputs are now statically-typed and the types used are the Scala types that correspond to the tensors’ TensorFlow data types. For example:\nval t1 = Tensor(0.5, 1) // The inferred type is Tensor[Double].\nval t2 = Tensor(1, 2)   // The inferred type is Tensor[Int].\nval t3 = t1 + t2        // The inferred type is Tensor[Double].\nval t4 = t3.isNaN       // The inferred type is Tensor[Boolean].\nval t5 = t3.any()       // Fails at compile-time because `any()` is only\n                        // supported for Tensor[Boolean].\nA similar situation now applies to Outputs. Ops are also typed and so is the auto-differentiation implementation.\nThis resulted in major simplifications in the data pipeline and the high level learn API. Datasets and dataset iterators do not “carry” T, V, D, and S types with them now, but rather just the type of the elements they contain/produce.\nA new type trait called TF is also introduced that denotes supported Scala types in TensorFlow (e.g., TF[Int] and TF[Float]). Similarly, some more type traits are introduced to denote type constraints for various ops (e.g., IsIntOrUInt[Int], IsIntOrUInt[Long], IsFloatOrDouble[Float], etc.). These type traits are powered by a general implementation of union types for Scala.\nOther new features include: - data module: - Added support for the mapAndBatch transformation.","title":"Release 0.4.0"},{"location":"/release_notes/0.3.0.html","text":"","title":"Release 0.3.0"},{"location":"/release_notes/0.3.0.html#release-0-3-0","text":"With this release we have finally added support for static data type information for tensors (not for symbolic tensors yet though – for now we effectively have support for a statically-typed version of numpy for Scala). This is an important milestone and contributes significantly to type safety, which can help catch errors at compile time, rather than runtime. For example:\nval t1 = Tensor(0.5, 1) // The inferred type is Tensor[FLOAT64].\nval t2 = Tensor(1, 2)   // The inferred type is Tensor[INT32].\nval t3 = t1 + t2        // The inferred type is Tensor[FLOAT64].\nval t4 = t3.isNaN       // The inferred type is Tensor[BOOLEAN].\nval t5 = t3.any()       // Fails at compile-time because `any()` is only\n                        // supported for Tensor[BOOLEAN].\nOther new features include:\nImprovements to the high-level learn API: Layers can now provide and use their own parameter generator, and can also access the current training step (using Layer.currentStep). Layers now support .map(...). Added support for batch normalization. Added support for tf.logSigmoid and tf.lrn. Added support for the following new metrics: Grouped precision. Precision-at-k. data module: Added support for loading the extreme classification repository datasets (i.e., data.XCLoader). Added support for randomly splitting datasets.","title":"Release 0.3.0"},{"location":"/release_notes/0.2.4.html","text":"","title":"Release 0.2.4"},{"location":"/release_notes/0.2.4.html#release-0-2-4","text":"Fixed an issue with the packaged pre-compiled TensorFlow binaries that affected Linux platforms.","title":"Release 0.2.4"},{"location":"/release_notes/0.2.3.html","text":"","title":"Release 0.2.3"},{"location":"/release_notes/0.2.3.html#release-0-2-3","text":"Added compatibility with TensorFlow 1.9-rc1.","title":"Release 0.2.3"},{"location":"/release_notes/0.2.2.html","text":"","title":"Release 0.2.2"},{"location":"/release_notes/0.2.2.html#release-0-2-2","text":"In this release we have updated the precompiled TensorFlow binaries distributed with this library.","title":"Release 0.2.2"},{"location":"/release_notes/0.2.1.html","text":"","title":"Release 0.2.1"},{"location":"/release_notes/0.2.1.html#release-0-2-1","text":"In this release we have fixed an issue related to the packaging and distributing of the pre-compiled TensorFlow shared libraries.","title":"Release 0.2.1"},{"location":"/release_notes/0.2.0.html","text":"","title":"Release 0.2.0"},{"location":"/release_notes/0.2.0.html#release-0-2-0","text":"In this release we have:\nAdded support for incremental compilation. Added support for Horovod. Added support for timelines to allow for easy profiling of TensorFlow graphs. Fixed a major memory leak (issue #87). Updated the JNI bindings to be compatible with the TensorFlow 1.9.0 release. Added support for obtaining the list of available devices from within Scala. Fixed bugs for some control flow ops. Added support for tf.cases. Added support for the RMSProp optimizer, the lazy Adam optimizer, the AMSGrad optimizer, the lazy AMSGrad optimizer, and the YellowFin optimizer. Added more learning rate decay schemes: Cosine decay. Cycle-linear 10x decay. Square-root decay. More warm-up decay schedules. Added support for dataset interleave ops. Fixed some bugs related to variable scopes and variable sharing. Fixed some bugs related to functional ops. Added support for some new image-related ops, under the namespace tf.image. Improved consistency for the creation of initializer ops. Added support for the tf.initializer op creation context. Exposed part of the TensorArray API. Exposed tf.Op.Builder in the public API. Improvements to the learn API: Refactored mode into an implicit argument. Improved the evaluator hook. Removed the layer creation context mechanism, to be refactored later. It was causing some issues due to bad design and unclear semantics. The plan is to implement this, in the near future, as wrapper creation context layers. Improved the Model class. Fixed a bug that was causing some issues related to inference hooks in the in-memory estimator. Improved logging. Added support for reading and writing numpy (i.e., .npy) files. Added a logo. :)","title":"Release 0.2.0"},{"location":"/release_notes/0.1.1.html","text":"","title":"Release 0.1.1"},{"location":"/release_notes/0.1.1.html#release-0-1-1","text":"This release fixes the following bugs:\nIssue with the packaged pre-compiled TensorFlow binaries that affected Linux platforms. Learn API bug where the shared name of input iterators was being set incorrectly.\nI also switched to using CircleCI for continuous integration, instead of TravisCI.","title":"Release 0.1.1"},{"location":"/release_notes/0.1.0.html","text":"","title":"Release 0.1.0"},{"location":"/release_notes/0.1.0.html#release-0-1-0","text":"This is the first official release of TensorFlow for Scala. The library website will soon be updated with information about the functionality supported by this API. Most of the main TensorFlow Python API functionality is already supported.","title":"Release 0.1.0"}]}